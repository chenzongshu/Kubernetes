# network namespace

容器使用namespace做资源隔离, 其中network namespace来做网络资源隔离, 可以使用 `ip netns list`来查看当前network namespace

一个 Network Namespace 的网络栈包括：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。

对应的network namespace操作命令已经集成到Linux ip命令的子命令 `ip netns`中:

```
ip netns add netns1  #创建名为netns1的network namespace
ip netns exec netns1  #进入netns1
ip netns exec netns1 ip link list  #进入netns1执行ip link list
ip netns delete netns1  #删除netns1
```

## 查看不同进程是否在同一ns

每个linux进程都有一个 `/proc/{PID}/ns`目录, 这里面每个文件都代表一个类型的namespace

```
[root@localhost ns]# ls -l
total 0
lrwxrwxrwx 1 root root 0 Jan 27 12:22 ipc -> ipc:[4026531839]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 mnt -> mnt:[4026531840]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 net -> net:[4026531956]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 pid -> pid:[4026531836]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 uts -> uts:[4026531838]
```

如果两个进程属于同一个ns, 对应ns后面的inode数字(即[4026531956])相同

## ns的绑定和解绑

通过Linux系统的系统调用 `setns()` 把一个进程计入一个已经存在的 ns中

通过系统调用 `unshare()` 把进程移出ns中

# veth pair

veth是虚拟以太网卡的缩写(virtual Ethernet)的缩写, 总是成对出现, 所以称为veth pair, 常用于跨namespace之间通信

## 创建

创建veth pair, 名字分别是veth0和veth1

```
ip link add veth0 type veth peer name veth1
```

## 查看

```
ip link list

5: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 12:28:7c:43:8f:e6 brd ff:ff:ff:ff:ff:ff
6: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 62:0f:93:c0:86:8d brd ff:ff:ff:ff:ff:ff
```

初始创建的veth pair表现为2块网卡, 初始状态是down, 可以通过`ip link`设置为up

```
ip link set dev veth0 up
```

veth pair同样可以配置IP地址

```
ifconfig veth0 10.20.30.40/24
```

将veth pair放入namespace中

```
ip link set veth0 netns newns
```

## 查看容器和宿主机对应的veth pair

如果容器内没有安装对应的工具, 简单方式可以**进入容器内**查看iflink的序号, 可以看到是11

```
root@3f555d52d6eb:/# cat /sys/class/net/eth0/iflink
11
```

然后再宿主机上, 查看到11的veth, 这两个就是一对

```
[root@localhost ~]# ip link show

......
11: vethd71cf35@if10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether 12:a6:06:45:2a:20 brd ff:ff:ff:ff:ff:ff link-netnsid 0
```

注意, `vethd71cf35@if10` 这种格式的名称, 后面的 `@if10`, 其中10就是和它成对的veth pair的index

如果镜像中有网络工具, 可以在容器中使用 `ip link show eth0` 或者 `ethtool -S`来显示

# tun/tap设备

什么是 tun/tap设备, 从网络虚拟化的角度来说, 它是虚拟网卡, 一端连着网络协议栈, 一端连着用户态程序. TUN 设备的功能非常简单，即：**在操作系统内核和用户应用程序之间传递 IP 包**

但是他们还是有些区别的, tun表示虚拟的是点对点设备, tap表示虚拟的以太网设备, 这两种设备针对的网络包实施不同的封装

tun/tap设备可以将TCP/IP协议栈处理好的网络包发给任何一个使用tun/tap驱动的进程, 由进程重新处理后发到物理链路中. tun/tap设备就像埋在用户程序空间的一个钩子, 我们可以很方便的将对网络包的处理程序挂在这个钩子上

从网络协议栈的角度看, tun/tap设备这类虚拟网卡和物理网卡没有区别, 只是对tun/tap设备而言, 它与物理网卡的不同之处在于**它的数据源不是物理链路, 而是来自于用户态!**, 所哦以tun/tap设备就是利用Linux的设备文件实现内核态和用户态数据交换

普通物理网卡是通过网线收发数据包, 而tun设备通过一个设备文件(/dev/tunX)收发数据包, 所有对这个文件的写操作都会通过tun设备转换成一个数据包发给内核网络协议栈, 当内核发一个包给tun设备的时候, 用户态进程可以读取这个文件拿到包内容.

tun和tap设备区别:

- tun设备的 /dev/tunX 文件收发的是IP包, 因此只能工作在L3, 无法和物理网卡做桥接, 但可以通过三层交换,(例如 ip_forward)与物理网卡连通;
- tap设备的/dev/tapX 文件收发的是链路层数据包, 可以与物理网卡桥接

# VXLAN

VXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）

VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。

而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）

VTEP 设备的作用，就是对数据的封包解包。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。

## Flannel VxLan

下面简要的以Flannel说明下Vxlan模式下怎么把数据包转发出去的

1. Flannel在启动的时候会给每个机器启动一个VTEP设备, 名字是flannel.1, 它既有 IP 地址，也有 MAC 地址

2. 从PodA的IP x.x.15.A 访问PodB的IP x.x.16.B的时候, 先到docker0网桥, 然后被路由到flannel.1, 也就是到了"隧道"的入口

怎么找到PodB锁在节点的VTEP设备呢? 原来NodeB的flannel在启动的时候, 就会给每个节点增加一条路由:

```
$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
...
x.x.16.0       x.x.16.0       255.255.255.0   UG    0      0        0 flannel.1
```

这条规则的意思是：凡是发往 x.x.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：x.x.16.0。而x.x.16.0正是 NodeB 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址

3. VXLAN数据包会加上 `VXLAN头(VNI)`+`目的VTEP设备的MAC地址`+`目的容器的IP地址`的头, 上一步我们只知道目的VTEP的IP地址, 这个时候就需要从ARP表里面获取对应的MAC地址, 对应的记录也是NodeB启动的时候Flannel进程添加到每个节点的ARP表里面的, 可以用`ip neigh show dev flannel.1`命令来查看

4. 通过UDP协议发包

   实际上现在源端只知道对端的VTEP设备的MAC和IP地址, 不知道节点的IP地址, 这个时候就需要flannel.1设备扮演网桥的角色, 在二层网络进行 UDP 包的转发, Linux内核通过FDB（Forwarding Database）的转发数据库, 来知道对应的Node IP

   ```
   # 在 Node A 上，使用“目的 VTEP 设备”的 MAC 地址进行查询
   $ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37
   5e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent
   ```

   可以看到，在上面这条 FDB 记录里，指定了这样一条规则，即：

   发往我们前面提到的“目的 VTEP 设备”（MAC 地址是 5e:f8:4f:00:e3:37）的二层数据帧，应该通过 flannel.1 设备，发往 IP 地址为 10.168.0.3 的主机。显然，这台主机正是 Node B，UDP 包要发往的目的地就找到了。

5. 然后最外层还需要封装上目的节点的IP和MAC, 成为普通的UDP包, 发到对端. 对端经过网络协议栈的拆包, 发现VXLAN头, 并且VNI为1, 转给flannel.1(因为VNI为1, 所有所有的VTEP设备都叫flannel.1), 然后flannel.1进一步拆包, 发到容器的network namesapce中

#  Kubernetes CNI

注意Kubernetes没有使用docker0这个网桥, 主要是一是k8s没有使用CNM网络模型, 二是与k8s配置Pod是通过pause容器密切相关

## CNI安装配置

### 安装

我们在部署 Kubernetes 的时候，有一个步骤是安装 kubernetes-cni 包，它的目的就是在宿主机上安装**CNI 插件所需的基础可执行文件**。

在安装完成后，你可以在宿主机的 /opt/cni/bin 目录下看到它们，如下所示：

```
$ ls -al /opt/cni/bin/
total 73088
-rwxr-xr-x 1 root root  3890407 Aug 17  2017 bridge
-rwxr-xr-x 1 root root  9921982 Aug 17  2017 dhcp
-rwxr-xr-x 1 root root  2814104 Aug 17  2017 flannel
-rwxr-xr-x 1 root root  2991965 Aug 17  2017 host-local
-rwxr-xr-x 1 root root  3475802 Aug 17  2017 ipvlan
-rwxr-xr-x 1 root root  3026388 Aug 17  2017 loopback
-rwxr-xr-x 1 root root  3520724 Aug 17  2017 macvlan
-rwxr-xr-x 1 root root  3470464 Aug 17  2017 portmap
-rwxr-xr-x 1 root root  3877986 Aug 17  2017 ptp
-rwxr-xr-x 1 root root  2605279 Aug 17  2017 sample
-rwxr-xr-x 1 root root  2808402 Aug 17  2017 tuning
-rwxr-xr-x 1 root root  3475750 Aug 17  2017 vlan
```

这些 CNI 的基础可执行文件，按照功能可以分为三类：

**第一类，叫作 Main 插件，它是用来创建具体网络设备的二进制文件**。比如，bridge（网桥设备）、ipvlan、loopback（lo 设备）、macvlan、ptp（Veth Pair 设备），以及 vlan。

我在前面提到过的 Flannel、Weave 等项目，都属于“网桥”类型的 CNI 插件。所以在具体的实现中，它们往往会调用 bridge 这个二进制文件。这个流程，我马上就会详细介绍到。

**第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件**。比如，dhcp，这个文件会向 DHCP 服务器发起请求；host-local，则会使用预先配置的 IP 地址段来进行分配。

**第三类，是由 CNI 社区维护的内置 CNI 插件**。比如：flannel，就是专门为 Flannel 项目提供的 CNI 插件；tuning，是一个通过 sysctl 调整网络设备参数的二进制文件；portmap，是一个通过 iptables 配置端口映射的二进制文件；bandwidth，是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件。

### 配置

CNI配置文件会生成在`/etc/cni/net.d`里面, Kubernetes 目前不支持多个 CNI 插件混用。如果你在 CNI 配置目录（/etc/cni/net.d）里放置了多个 CNI 配置文件的话，dockershim 只会加载按字母顺序排序的第一个插件

需要注意的是，在 Kubernetes 中，处理容器网络相关的逻辑并不会在 kubelet 主干代码里执行，而是会在具体的 CRI（Container Runtime Interface，容器运行时接口）实现里完成。对于 Docker 项目来说，它的 CRI 实现叫作 dockershim，你可以在 kubelet 的代码里找到它。

 ### 工作原理

以flannel为例, 说明下CNI的工作原理

当 kubelet 组件需要创建 Pod 的时候，它第一个创建的一定是 Infra 容器。所以在这一步，dockershim 就会先调用 Docker API 创建并启动 Infra 容器，紧接着执行一个叫作 SetUpPod 的方法。这个方法的作用就是：为 CNI 插件准备参数，然后调用 CNI 插件为 Infra 容器配置网络。

这里要调用的 CNI 插件，就是 /opt/cni/bin/flannel；而调用它所需要的参数，分为两部分。

**第一部分，是由 dockershim 设置的一组 CNI 环境变量。**

其中，最重要的环境变量参数叫作：CNI_COMMAND。它的取值只有两种：ADD 和 DEL。

**这个 ADD 和 DEL 操作，就是 CNI 插件唯一需要实现的两个方法。**

其中 ADD 操作的含义是：把容器添加到 CNI 网络里；DEL 操作的含义则是：把容器从 CNI 网络里移除掉。

而对于网桥类型的 CNI 插件来说，这两个操作意味着把容器以 Veth Pair 的方式“插”到 CNI 网桥上，或者从网桥上“拔”掉。

CNI 的 ADD 操作需要的参数包括：容器里网卡的名字 eth0（CNI_IFNAME）、Pod 的 Network Namespace 文件的路径（CNI_NETNS）、容器的 ID（CNI_CONTAINERID）等。这些参数都属于上述环境变量里的内容。其中，Pod（Infra 容器）的 Network Namespace 文件的路径，即：/proc/< 容器进程的 PID>/ns/net。

除此之外，在 CNI 环境变量里，还有一个叫作 CNI_ARGS 的参数。通过这个参数，CRI 实现（比如 dockershim）就可以以 Key-Value 的格式，传递自定义信息给网络插件。这是用户将来自定义 CNI 协议的一个重要方法。

**第二部分，则是 dockershim 从 CNI 配置文件里加载到的、默认插件的配置信息。**

这个配置信息在 CNI 中被叫作 Network Configuration，它的完整定义你可以参考[这个文档](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration)。dockershim 会把 Network Configuration 以 JSON 数据的格式，通过标准输入（stdin）的方式传递给 Flannel CNI 插件。

Flannel CNI 插件对 dockershim 传来的 Network Configuration 进行补充, 然后就会调用 CNI bridge 插件

首先，CNI bridge 插件会在宿主机上检查 CNI 网桥是否存在。如果没有的话，那就创建它。这相当于在宿主机上执行：

```bash
# 在宿主机上
$ ip link add cni0 type bridge
$ ip link set cni0 up
```

接下来，CNI bridge 插件会通过 Infra 容器的 Network Namespace 文件，进入到这个 Network Namespace 里面，然后创建一对 Veth Pair 设备。

紧接着，它会把这个 Veth Pair 的其中一端，“移动”到宿主机上。这相当于在容器里执行如下所示的命令：

```
# 在容器里

# 创建一对 Veth Pair 设备。其中一个叫作 eth0，另一个叫作 vethb4963f3
$ ip link add eth0 type veth peer name vethb4963f3

# 启动 eth0 设备
$ ip link set eth0 up 

# 将 Veth Pair 设备的另一端（也就是 vethb4963f3 设备）放到宿主机（也就是 Host Namespace）里
$ ip link set vethb4963f3 netns $HOST_NS

# 通过 Host Namespace，启动宿主机上的 vethb4963f3 设备
$ ip netns exec $HOST_NS ip link set vethb4963f3 up 
```

这样，vethb4963f3 就出现在了宿主机上，而且这个 Veth Pair 设备的另一端，就是容器里面的 eth0。

接下来，CNI bridge 插件就可以把 vethb4963f3 设备连接在 CNI 网桥上。这相当于在宿主机上执行：

```
# 在宿主机上
$ ip link set vethb4963f3 master cni0
```

在将 vethb4963f3 设备连接在 CNI 网桥之后，CNI bridge 插件还会为它设置**Hairpin Mode（发夹模式）**。这是因为，在默认情况下，网桥设备是不允许一个数据包从一个端口进来后，再从这个端口发出去的。但是，它允许你为这个端口开启 Hairpin Mode，从而取消这个限制。

接下来，CNI bridge 插件会调用 CNI ipam 插件，从 ipam.subnet 字段规定的网段里为容器分配一个可用的 IP 地址。然后，CNI bridge 插件就会把这个 IP 地址添加在容器的 eth0 网卡上，同时为容器设置默认路由。这相当于在容器里执行：

```
# 在容器里
$ ip addr add 10.244.0.2/24 dev eth0
$ ip route add default via 10.244.0.1 dev eth0
```

最后，CNI bridge 插件会为 CNI 网桥添加 IP 地址。这相当于在宿主机上执行：

```
# 在宿主机上
$ ip addr add 10.244.0.1/24 dev cni0
```

在执行完上述操作之后，CNI 插件会把容器的 IP 地址等信息返回给 dockershim，然后被 kubelet 添加到 Pod 的 Status 字段。

需要注意的是，对于非网桥类型的 CNI 插件，上述“将容器添加到 CNI 网络”的操作流程，以及网络方案本身的工作原理，就都不太一样了。

## 三层网络

如果是Flannel Host-gw或者Calico BGP模式, 是通过三层网络来实现的, 具体的实现都是类似的

都会在每个主机上添加一个格式如下所示的路由规则：

```
< 目的容器 IP 地址段 > via < 网关的 IP 地址 > dev eth0
```

其中，网关的 IP 地址，正是目的容器所在宿主机的 IP 地址。

即**通过路由把每个容器子网段的IP指向了对应主机的IP , 然后每个主机上又把自己子网段的IP, 指向CNI网桥, 从而进入容器内部**

不同的是, Flannel是通过Etcd来保存主机信息的, 然后通过watch这些信息来更新路由表, Calico通过BGP来自动分发路由表, **而且Calico不会在宿主机上创出任何网桥设备**

注意, Calico设置最核心的“下一跳”路由规则，是由 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的

而这些通过 BGP 协议传输的消息，你可以简单地理解为如下格式：

```
[BGP 消息]
我是宿主机 192.168.1.2
10.233.2.0/24 网段的容器都在我这里
这些容器的下一跳地址是我
```

不难发现，Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。

